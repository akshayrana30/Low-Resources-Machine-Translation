{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Transformers_Google import *\n",
    "import dataloaders_git as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, fr = d.create_dataset(\"./pairs_en_fr.txt\")\n",
    "input_tensor, inp_lang_tokenizer = d.tokenize(list(en))\n",
    "target_tensor, targ_lang_tokenizer = d.tokenize(list(fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_tensor_train, \\\n",
    "input_tensor_val, \\\n",
    "target_tensor_train, \\\n",
    "target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=1234)\n",
    "\n",
    "## Keeping gold for use later.. \n",
    "en_train, \\\n",
    "en_val, \\\n",
    "fr_train, \\\n",
    "fr_val = train_test_split(en, fr, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE*2, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4 #6\n",
    "d_model = 256  #512\n",
    "dff = 1024      #2048\n",
    "num_heads = 8  \n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.word_index)+1\n",
    "target_vocab_size = len(targ_lang_tokenizer.word_index)+1\n",
    "dropout_rate = 0.1\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='val_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#   print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "# train_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "# ]\n",
    "\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "    \n",
    "    \n",
    "def val_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 False, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "        \n",
    "    val_loss(loss)\n",
    "    val_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "def evaluate_batch(inp_tensor):\n",
    "  # Expecting input from the val_dataset which is already tokenised.\n",
    "  \n",
    "    encoder_input = tf.convert_to_tensor(inp_tensor)\n",
    "    decoder_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE*2, axis=1)\n",
    "    output = decoder_input\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token for all the batches\n",
    "        if (predicted_id == targ_lang_tokenizer.word_index['<end>']).numpy().all():\n",
    "            return output, attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def translate_batch(inp, tar):\n",
    "    output,_ = evaluate_batch(inp)\n",
    "    pred_sentences = targ_lang_tokenizer.sequences_to_texts(output.numpy())\n",
    "    pred_sentences = [x.split(\"<end>\")[0].replace(\"<start>\",\"\").strip() for x in pred_sentences]\n",
    "    gold_sentences = targ_lang_tokenizer.sequences_to_texts(tar.numpy())\n",
    "    gold_sentences = [x.replace('<start> ', \"\").replace(' <end>', \"\").replace('<OOV>', \"\").strip() for x in gold_sentences]\n",
    "    return gold_sentences, pred_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Training--\n",
      "Epoch 1 Batch 0 Train Loss 9.8063 Train Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Train Loss 9.7243 Train Accuracy 0.0031\n",
      "Epoch 1 Batch 100 Train Loss 9.5925 Train Accuracy 0.0057\n",
      "Epoch 1 Train Loss 9.4850 Train Accuracy 0.0064 Val Loss 9.0125 Val Accuracy 0.0086\n",
      "Time taken for 1 epoch: 133.52070951461792 secs\n",
      "\n",
      "Epoch 2 Batch 0 Train Loss 9.0149 Train Accuracy 0.0088\n",
      "Epoch 2 Batch 50 Train Loss 8.7858 Train Accuracy 0.0101\n",
      "Epoch 2 Batch 100 Train Loss 8.5141 Train Accuracy 0.0119\n",
      "Epoch 2 Train Loss 8.3083 Train Accuracy 0.0130\n",
      "Time taken for 1 epoch: 119.84666609764099 secs\n",
      "\n",
      "Epoch 3 Batch 0 Train Loss 7.4687 Train Accuracy 0.0171\n",
      "Epoch 3 Batch 50 Train Loss 7.2187 Train Accuracy 0.0177\n",
      "Epoch 3 Batch 100 Train Loss 7.0253 Train Accuracy 0.0194\n",
      "Epoch 3 Train Loss 6.9297 Train Accuracy 0.0204\n",
      "Time taken for 1 epoch: 119.87369441986084 secs\n",
      "\n",
      "Epoch 4 Batch 0 Train Loss 6.5702 Train Accuracy 0.0221\n",
      "Epoch 4 Batch 50 Train Loss 6.5141 Train Accuracy 0.0232\n",
      "Epoch 4 Batch 100 Train Loss 6.4614 Train Accuracy 0.0236\n",
      "Epoch 4 Train Loss 6.4258 Train Accuracy 0.0238\n",
      "Time taken for 1 epoch: 119.70246362686157 secs\n",
      "\n",
      "Epoch 5 Batch 0 Train Loss 6.2008 Train Accuracy 0.0265\n",
      "Epoch 5 Batch 50 Train Loss 6.1775 Train Accuracy 0.0266\n",
      "Epoch 5 Batch 100 Train Loss 6.1148 Train Accuracy 0.0275\n",
      "Epoch 5 Train Loss 6.0773 Train Accuracy 0.0280 Val Loss 6.0480 Val Accuracy 0.0294\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Time taken for 1 epoch: 133.05303859710693 secs\n",
      "\n",
      "Epoch 6 Batch 0 Train Loss 5.8269 Train Accuracy 0.0307\n",
      "Epoch 6 Batch 50 Train Loss 5.8227 Train Accuracy 0.0303\n",
      "Epoch 6 Batch 100 Train Loss 5.7995 Train Accuracy 0.0305\n",
      "Epoch 6 Train Loss 5.7811 Train Accuracy 0.0305\n",
      "Time taken for 1 epoch: 117.6855161190033 secs\n",
      "\n",
      "Epoch 7 Batch 0 Train Loss 5.6085 Train Accuracy 0.0318\n",
      "Epoch 7 Batch 50 Train Loss 5.6024 Train Accuracy 0.0321\n",
      "Epoch 7 Batch 100 Train Loss 5.5803 Train Accuracy 0.0326\n",
      "Epoch 7 Train Loss 5.5563 Train Accuracy 0.0332\n",
      "Time taken for 1 epoch: 117.5123016834259 secs\n",
      "\n",
      "Epoch 8 Batch 0 Train Loss 5.3791 Train Accuracy 0.0330\n",
      "Epoch 8 Batch 50 Train Loss 5.3806 Train Accuracy 0.0361\n",
      "Epoch 8 Batch 100 Train Loss 5.3511 Train Accuracy 0.0364\n",
      "Epoch 8 Train Loss 5.3410 Train Accuracy 0.0367\n",
      "Time taken for 1 epoch: 118.2988543510437 secs\n",
      "\n",
      "Epoch 9 Batch 0 Train Loss 5.2381 Train Accuracy 0.0372\n",
      "Epoch 9 Batch 50 Train Loss 5.1508 Train Accuracy 0.0406\n",
      "Epoch 9 Batch 100 Train Loss 5.1276 Train Accuracy 0.0405\n",
      "Epoch 9 Train Loss 5.1194 Train Accuracy 0.0408\n",
      "Time taken for 1 epoch: 118.10326409339905 secs\n",
      "\n",
      "Epoch 10 Batch 0 Train Loss 4.9537 Train Accuracy 0.0455\n",
      "Epoch 10 Batch 50 Train Loss 4.9347 Train Accuracy 0.0435\n",
      "Epoch 10 Batch 100 Train Loss 4.9201 Train Accuracy 0.0442\n",
      "Epoch 10 Train Loss 4.9081 Train Accuracy 0.0442 Val Loss 5.2966 Val Accuracy 0.0441\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Time taken for 1 epoch: 131.40041971206665 secs\n",
      "\n",
      "--Saving files to get Bleu Scores--\n",
      "Batch Size for Evaluation 128\n",
      "Evaluating for batch 0\n",
      "Evaluating for batch 5\n",
      "Evaluating for batch 10\n",
      "Evaluating for batch 15\n",
      "Time taken for Evaluation: 886.4375915527344 secs\n",
      "\n",
      "Files saved: get_bleu_gold_epoch_10.txt\n",
      "-------------\n",
      "Epoch 11 Batch 0 Train Loss 4.8052 Train Accuracy 0.0485\n",
      "Epoch 11 Batch 50 Train Loss 4.7232 Train Accuracy 0.0474\n",
      "Epoch 11 Batch 100 Train Loss 4.7107 Train Accuracy 0.0476\n",
      "Epoch 11 Train Loss 4.7112 Train Accuracy 0.0476\n",
      "Time taken for 1 epoch: 117.35760855674744 secs\n",
      "\n",
      "Epoch 12 Batch 0 Train Loss 4.4145 Train Accuracy 0.0507\n",
      "Epoch 12 Batch 50 Train Loss 4.5295 Train Accuracy 0.0508\n",
      "Epoch 12 Batch 100 Train Loss 4.5345 Train Accuracy 0.0507\n",
      "Epoch 12 Train Loss 4.5357 Train Accuracy 0.0506\n",
      "Time taken for 1 epoch: 117.2822892665863 secs\n",
      "\n",
      "Epoch 13 Batch 0 Train Loss 4.2217 Train Accuracy 0.0530\n",
      "Epoch 13 Batch 50 Train Loss 4.3722 Train Accuracy 0.0538\n",
      "Epoch 13 Batch 100 Train Loss 4.3751 Train Accuracy 0.0537\n",
      "Epoch 13 Train Loss 4.3702 Train Accuracy 0.0533\n",
      "Time taken for 1 epoch: 118.00150156021118 secs\n",
      "\n",
      "Epoch 14 Batch 0 Train Loss 4.2564 Train Accuracy 0.0615\n",
      "Epoch 14 Batch 50 Train Loss 4.2003 Train Accuracy 0.0563\n",
      "Epoch 14 Batch 100 Train Loss 4.2028 Train Accuracy 0.0562\n",
      "Epoch 14 Train Loss 4.2123 Train Accuracy 0.0562\n",
      "Time taken for 1 epoch: 118.24427008628845 secs\n",
      "\n",
      "Epoch 15 Batch 0 Train Loss 4.0000 Train Accuracy 0.0589\n",
      "Epoch 15 Batch 50 Train Loss 4.0104 Train Accuracy 0.0592\n",
      "Epoch 15 Batch 100 Train Loss 4.0428 Train Accuracy 0.0590\n",
      "Epoch 15 Train Loss 4.0529 Train Accuracy 0.0588 Val Loss 5.0448 Val Accuracy 0.0506\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Time taken for 1 epoch: 131.4691390991211 secs\n",
      "\n",
      "Epoch 16 Batch 0 Train Loss 3.7979 Train Accuracy 0.0626\n",
      "Epoch 16 Batch 50 Train Loss 3.8588 Train Accuracy 0.0619\n",
      "Epoch 16 Batch 100 Train Loss 3.8873 Train Accuracy 0.0616\n",
      "Epoch 16 Train Loss 3.8953 Train Accuracy 0.0617\n",
      "Time taken for 1 epoch: 118.13933634757996 secs\n",
      "\n",
      "Epoch 17 Batch 0 Train Loss 3.7288 Train Accuracy 0.0646\n",
      "Epoch 17 Batch 50 Train Loss 3.6816 Train Accuracy 0.0644\n",
      "Epoch 17 Batch 100 Train Loss 3.7234 Train Accuracy 0.0642\n",
      "Epoch 17 Train Loss 3.7415 Train Accuracy 0.0643\n",
      "Time taken for 1 epoch: 117.31874489784241 secs\n",
      "\n",
      "Epoch 18 Batch 0 Train Loss 3.4999 Train Accuracy 0.0690\n",
      "Epoch 18 Batch 50 Train Loss 3.5313 Train Accuracy 0.0680\n",
      "Epoch 18 Batch 100 Train Loss 3.5520 Train Accuracy 0.0678\n",
      "Epoch 18 Train Loss 3.5727 Train Accuracy 0.0676\n",
      "Time taken for 1 epoch: 118.15744066238403 secs\n",
      "\n",
      "Epoch 19 Batch 0 Train Loss 3.2228 Train Accuracy 0.0686\n",
      "Epoch 19 Batch 50 Train Loss 3.3420 Train Accuracy 0.0721\n",
      "Epoch 19 Batch 100 Train Loss 3.3833 Train Accuracy 0.0711\n",
      "Epoch 19 Train Loss 3.4067 Train Accuracy 0.0707\n",
      "Time taken for 1 epoch: 117.73321604728699 secs\n",
      "\n",
      "Epoch 20 Batch 0 Train Loss 3.0069 Train Accuracy 0.0694\n",
      "Epoch 20 Batch 50 Train Loss 3.1628 Train Accuracy 0.0758\n",
      "Epoch 20 Batch 100 Train Loss 3.2021 Train Accuracy 0.0754\n",
      "Epoch 20 Train Loss 3.2241 Train Accuracy 0.0747 Val Loss 5.0059 Val Accuracy 0.0556\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Time taken for 1 epoch: 129.45449590682983 secs\n",
      "\n",
      "--Saving files to get Bleu Scores--\n",
      "Batch Size for Evaluation 128\n",
      "Evaluating for batch 0\n",
      "Evaluating for batch 5\n",
      "Evaluating for batch 10\n",
      "Evaluating for batch 15\n",
      "Time taken for Evaluation: 854.1044800281525 secs\n",
      "\n",
      "Files saved: get_bleu_gold_epoch_20.txt\n",
      "-------------\n",
      "Epoch 21 Batch 0 Train Loss 3.0332 Train Accuracy 0.0852\n",
      "Epoch 21 Batch 50 Train Loss 2.9644 Train Accuracy 0.0806\n",
      "Epoch 21 Batch 100 Train Loss 3.0036 Train Accuracy 0.0802\n",
      "Epoch 21 Train Loss 3.0367 Train Accuracy 0.0796\n",
      "Time taken for 1 epoch: 114.89527630805969 secs\n",
      "\n",
      "Epoch 22 Batch 0 Train Loss 2.7845 Train Accuracy 0.0936\n",
      "Epoch 22 Batch 50 Train Loss 2.7561 Train Accuracy 0.0880\n",
      "Epoch 22 Batch 100 Train Loss 2.8106 Train Accuracy 0.0859\n",
      "Epoch 22 Train Loss 2.8481 Train Accuracy 0.0846\n",
      "Time taken for 1 epoch: 115.12283635139465 secs\n",
      "\n",
      "Epoch 23 Batch 0 Train Loss 2.5326 Train Accuracy 0.0908\n",
      "Epoch 23 Batch 50 Train Loss 2.5611 Train Accuracy 0.0933\n",
      "Epoch 23 Batch 100 Train Loss 2.6198 Train Accuracy 0.0912\n",
      "Epoch 23 Train Loss 2.6477 Train Accuracy 0.0903\n",
      "Time taken for 1 epoch: 115.13966202735901 secs\n",
      "\n",
      "Epoch 24 Batch 0 Train Loss 2.4004 Train Accuracy 0.1044\n",
      "Epoch 24 Batch 50 Train Loss 2.3345 Train Accuracy 0.1003\n",
      "Epoch 24 Batch 100 Train Loss 2.3919 Train Accuracy 0.0985\n",
      "Epoch 24 Train Loss 2.4382 Train Accuracy 0.0972\n",
      "Time taken for 1 epoch: 115.3597297668457 secs\n",
      "\n",
      "Epoch 25 Batch 0 Train Loss 2.0495 Train Accuracy 0.1065\n",
      "Epoch 25 Batch 50 Train Loss 2.1253 Train Accuracy 0.1090\n",
      "Epoch 25 Batch 100 Train Loss 2.1919 Train Accuracy 0.1061\n",
      "Epoch 25 Train Loss 2.2351 Train Accuracy 0.1047 Val Loss 5.1946 Val Accuracy 0.0590\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
      "Time taken for 1 epoch: 128.0163698196411 secs\n",
      "\n",
      "Epoch 26 Batch 0 Train Loss 1.7981 Train Accuracy 0.1164\n",
      "Epoch 26 Batch 50 Train Loss 1.9228 Train Accuracy 0.1166\n",
      "Epoch 26 Batch 100 Train Loss 1.9913 Train Accuracy 0.1141\n",
      "Epoch 26 Train Loss 2.0323 Train Accuracy 0.1120\n",
      "Time taken for 1 epoch: 115.17693781852722 secs\n",
      "\n",
      "Epoch 27 Batch 0 Train Loss 1.5728 Train Accuracy 0.1228\n",
      "Epoch 27 Batch 50 Train Loss 1.7295 Train Accuracy 0.1234\n",
      "Epoch 27 Batch 100 Train Loss 1.8015 Train Accuracy 0.1209\n",
      "Epoch 27 Train Loss 1.8517 Train Accuracy 0.1188\n",
      "Time taken for 1 epoch: 115.17296957969666 secs\n",
      "\n",
      "Epoch 28 Batch 0 Train Loss 1.5497 Train Accuracy 0.1455\n",
      "Epoch 28 Batch 50 Train Loss 1.5420 Train Accuracy 0.1312\n",
      "Epoch 28 Batch 100 Train Loss 1.6130 Train Accuracy 0.1279\n",
      "Epoch 28 Train Loss 1.6621 Train Accuracy 0.1261\n",
      "Time taken for 1 epoch: 115.28688335418701 secs\n",
      "\n",
      "Epoch 29 Batch 0 Train Loss 1.3226 Train Accuracy 0.1514\n",
      "Epoch 29 Batch 50 Train Loss 1.4054 Train Accuracy 0.1374\n",
      "Epoch 29 Batch 100 Train Loss 1.4680 Train Accuracy 0.1345\n",
      "Epoch 29 Train Loss 1.5178 Train Accuracy 0.1318\n",
      "Time taken for 1 epoch: 115.38040328025818 secs\n",
      "\n",
      "Epoch 30 Batch 0 Train Loss 1.3150 Train Accuracy 0.1392\n",
      "Epoch 30 Batch 50 Train Loss 1.2565 Train Accuracy 0.1416\n",
      "Epoch 30 Batch 100 Train Loss 1.3312 Train Accuracy 0.1391\n",
      "Epoch 30 Train Loss 1.3740 Train Accuracy 0.1378 Val Loss 5.6450 Val Accuracy 0.0600\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
      "Time taken for 1 epoch: 127.85880517959595 secs\n",
      "\n",
      "--Saving files to get Bleu Scores--\n",
      "Batch Size for Evaluation 128\n",
      "Evaluating for batch 0\n",
      "Evaluating for batch 5\n",
      "Evaluating for batch 10\n",
      "Evaluating for batch 15\n",
      "Time taken for Evaluation: 851.9496960639954 secs\n",
      "\n",
      "Files saved: get_bleu_gold_epoch_30.txt\n",
      "-------------\n",
      "Epoch 31 Batch 0 Train Loss 1.0733 Train Accuracy 0.1478\n",
      "Epoch 31 Batch 50 Train Loss 1.1220 Train Accuracy 0.1497\n",
      "Epoch 31 Batch 100 Train Loss 1.1828 Train Accuracy 0.1469\n",
      "Epoch 31 Train Loss 1.2241 Train Accuracy 0.1445\n",
      "Time taken for 1 epoch: 115.0163164138794 secs\n",
      "\n",
      "Epoch 32 Batch 0 Train Loss 0.9400 Train Accuracy 0.1587\n",
      "Epoch 32 Batch 50 Train Loss 1.0087 Train Accuracy 0.1562\n",
      "Epoch 33 Batch 100 Train Loss 0.9350 Train Accuracy 0.1585\n",
      "Epoch 33 Train Loss 0.9727 Train Accuracy 0.1562\n",
      "Time taken for 1 epoch: 115.3051826953888 secs\n",
      "\n",
      "Epoch 34 Batch 0 Train Loss 0.8151 Train Accuracy 0.1697\n",
      "Epoch 34 Batch 50 Train Loss 0.7907 Train Accuracy 0.1654\n",
      "Epoch 34 Batch 100 Train Loss 0.8330 Train Accuracy 0.1624\n",
      "Epoch 34 Train Loss 0.8709 Train Accuracy 0.1609\n",
      "Time taken for 1 epoch: 115.06942749023438 secs\n",
      "\n",
      "Epoch 35 Batch 0 Train Loss 0.6495 Train Accuracy 0.1659\n",
      "Epoch 35 Batch 50 Train Loss 0.7057 Train Accuracy 0.1678\n",
      "Epoch 35 Batch 100 Train Loss 0.7507 Train Accuracy 0.1664\n",
      "Epoch 35 Train Loss 0.7830 Train Accuracy 0.1651 Val Loss 6.1759 Val Accuracy 0.0618\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
      "Time taken for 1 epoch: 128.1274929046631 secs\n",
      "\n",
      "Epoch 36 Batch 0 Train Loss 0.6034 Train Accuracy 0.1829\n",
      "Epoch 36 Batch 50 Train Loss 0.6223 Train Accuracy 0.1733\n",
      "Epoch 36 Batch 100 Train Loss 0.6750 Train Accuracy 0.1715\n",
      "Epoch 36 Train Loss 0.7057 Train Accuracy 0.1695\n",
      "Time taken for 1 epoch: 115.87205052375793 secs\n",
      "\n",
      "Epoch 37 Batch 0 Train Loss 0.5263 Train Accuracy 0.1923\n",
      "Epoch 37 Batch 50 Train Loss 0.5759 Train Accuracy 0.1770\n",
      "Epoch 37 Batch 100 Train Loss 0.6146 Train Accuracy 0.1746\n",
      "Epoch 37 Train Loss 0.6404 Train Accuracy 0.1729\n",
      "Time taken for 1 epoch: 114.14683723449707 secs\n",
      "\n",
      "Epoch 38 Batch 0 Train Loss 0.4812 Train Accuracy 0.1839\n",
      "Epoch 38 Batch 50 Train Loss 0.5260 Train Accuracy 0.1800\n",
      "Epoch 38 Batch 100 Train Loss 0.5638 Train Accuracy 0.1775\n",
      "Epoch 38 Train Loss 0.5893 Train Accuracy 0.1755\n",
      "Time taken for 1 epoch: 114.59597873687744 secs\n",
      "\n",
      "Epoch 39 Batch 0 Train Loss 0.4548 Train Accuracy 0.1757\n",
      "Epoch 39 Batch 50 Train Loss 0.4840 Train Accuracy 0.1832\n",
      "Epoch 39 Batch 100 Train Loss 0.5168 Train Accuracy 0.1788\n",
      "Epoch 39 Train Loss 0.5386 Train Accuracy 0.1785\n",
      "Time taken for 1 epoch: 114.55658483505249 secs\n",
      "\n",
      "Epoch 40 Batch 0 Train Loss 0.4290 Train Accuracy 0.1990\n",
      "Epoch 40 Batch 50 Train Loss 0.4403 Train Accuracy 0.1847\n",
      "Epoch 40 Batch 100 Train Loss 0.4716 Train Accuracy 0.1834\n",
      "Epoch 40 Train Loss 0.4956 Train Accuracy 0.1807 Val Loss 6.7209 Val Accuracy 0.0606\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
      "Time taken for 1 epoch: 127.28589534759521 secs\n",
      "\n",
      "--Saving files to get Bleu Scores--\n",
      "Batch Size for Evaluation 128\n",
      "Evaluating for batch 0\n",
      "Evaluating for batch 5\n",
      "Evaluating for batch 10\n",
      "Evaluating for batch 15\n",
      "Time taken for Evaluation: 848.5938160419464 secs\n",
      "\n",
      "Files saved: get_bleu_gold_epoch_40.txt\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"--Training--\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Train Loss {:.4f} Train Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    if epoch==0 or (epoch+1)%5==0:\n",
    "        for (batch, (inp, tar)) in enumerate(val_dataset):\n",
    "            val_step(inp, tar)\n",
    "        \n",
    "        print ('Epoch {} Train Loss {:.4f} Train Accuracy {:.4f} Val Loss {:.4f} Val Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result(),\n",
    "                                                val_loss.result(),                                                    \n",
    "                                                val_accuracy.result()))\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print ('Epoch {} Train Loss {:.4f} Train Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))    \n",
    "    \n",
    "    if (epoch+1)%5==0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "  \n",
    "\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        new_start = time.time()\n",
    "        print(\"--Saving files to get Bleu Scores--\")\n",
    "        print(\"Batch Size for Evaluation\", BATCH_SIZE*2)\n",
    "        gold_file_path = \"get_bleu_gold_epoch_\"+str(epoch+1)+\".txt\"\n",
    "        pred_file_path = \"get_bleu_preds_epoch_\"+str(epoch+1)+\".txt\"\n",
    "        with open(gold_file_path, 'w', encoding='utf-8', buffering=1) as gold_file, open(pred_file_path, 'w', encoding='utf-8', buffering=1) as pred_file:\n",
    "            for (batch, (inp, tar)) in enumerate(val_dataset):\n",
    "                if batch%5==0:\n",
    "                    print(\"Evaluating for batch\", batch)\n",
    "                gold_fr,pred_fr = translate_batch(inp, tar)\n",
    "                for g_fr,p_fr in zip(gold_fr, pred_fr):\n",
    "                    gold_file.write(g_fr.strip() + '\\n')\n",
    "                    pred_file.write(p_fr.strip() + '\\n')\n",
    "        \n",
    "        \n",
    "        print('Time taken for Evaluation: {} secs\\n'.format(time.time() - new_start))\n",
    "        print(\"Files saved:\", gold_file_path)\n",
    "        print(\"-------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = 3 #targ_lang_tokenizer.index_word[4]\n",
    "  end_token = 4 #[tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  sentence = preprocess_sentence(inp_sentence)\n",
    "\n",
    "  inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "\n",
    "#   inp_sentence = tf.convert_to_tensor(inputs)\n",
    "  encoder_input = tf.convert_to_tensor(inputs)#tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [targ_lang_tokenizer.word_index['<start>']]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "  result = ''\n",
    "  for i in range(max_length_targ):\n",
    "#     print(encoder_input.shape)\n",
    "#     print(encoder_input)\n",
    "#     print(output.shape)\n",
    "#     print(output)\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "    \n",
    "\n",
    "\n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == targ_lang_tokenizer.word_index['<end>']:\n",
    "      return result, tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    " \n",
    "    result += targ_lang_tokenizer.index_word[predicted_id[0].numpy()[0]] + ' '\n",
    "\n",
    "  return result, output, attention_weights\n",
    "\n",
    "\n",
    "def translate(sentence):\n",
    "  result, output, attention_plot = evaluate(sentence)\n",
    "  \n",
    "#   print('Input: %s' % (sentence))\n",
    "#   print('Predicted translation: {}'.format(result))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je veux que mon maison a mon maison . '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I live in my house.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "sentences = inp_lang_tokenizer.sequences_to_texts(input_tensor_val)\n",
    "count = 0\n",
    "with open('final_results_input_25_2k.txt', 'w', encoding='utf-8') as input_file, open('final_results_prediction_25_2k.txt', 'w', encoding='utf-8') as pred_file:\n",
    "    for sent in sentences:\n",
    "      count+=1\n",
    "      if count%50==0:\n",
    "        print(count)\n",
    "      if count>1000:  \n",
    "        sent = sent.replace('<start> ', \"\")\n",
    "        sent = sent.replace(' <end>', \"\")\n",
    "        sent = sent.replace('<OOV>', \"\")\n",
    "        input_file.write(sent.strip() + '\\n')\n",
    "        res = translate(sent)\n",
    "        pred_file.write(res.strip() + '\\n')\n",
    "      if count>1500:\n",
    "        break\n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
