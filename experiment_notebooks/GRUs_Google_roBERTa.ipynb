{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "notebook original tutorial: https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8P2-p29DCr5o",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code",
    "id": "tnxXKDjq3jEL",
    "outputId": "d590c7da-cf26-4a03-cff4-05452d206345",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 9.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 16.8MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 23.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=97c8ff9252736afb89e827745cfea2072530b6185b43098184c9595d8120e73f\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code",
    "id": "ouPlWHrXGtrE",
    "outputId": "8e72cb05-098b-4ee5-c703-41f0ae247262",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TvKT-uof38Y2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#check that files needed are in the right location\n",
    "from pathlib import Path\n",
    "\n",
    "colab_path = '/content/drive'\n",
    "\n",
    "training_data = '/pairs_fr_eng.txt'\n",
    "\n",
    "# this roBERTa model was trained on the English unaligned data in pytorch, then converted to tensorflow\n",
    "roberta_eng_model = '/English_small'\n",
    "\n",
    "#not used, placeholder if decide to put roBERTa french in GRU decoder\n",
    "roberta_fr_model = ''\n",
    "\n",
    "training_data_path = colab_path + \"/My Drive/Colab Notebooks/carocode\" + training_data\n",
    "roBERTa_eng_model_path = colab_path + \"/My Drive/Colab Notebooks/carocode\" + roberta_eng_model\n",
    "\n",
    "target_gold_path = Path(colab_path + \"/My Drive/Colab Notebooks/carocode\" + roberta_eng_model + \"_target_gold.txt\")\n",
    "target_predicted_path = Path(colab_path + \"/My Drive/Colab Notebooks/carocode\" + roberta_eng_model + \"_target_predicted.txt\")\n",
    "input_gold_path = Path(colab_path + \"/My Drive/Colab Notebooks/carocode\" + roberta_eng_model + \"_input_gold.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "dxpPN7qw5b2P",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "num_examples = 11000\n",
    "\n",
    "# when fine_tuning roBERTa, batch_size is 16. \n",
    "# when using roBERTa as a feature extractor, batch_size is 64\n",
    "BATCH_SIZE = 16\n",
    "embedding_dim = 100   #hidden dimension size \n",
    "units = 252 # to match BERT hidden dim (roBERTa_small = 252, roBERTa_large=768)\n",
    "\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "rd0jw-eC3jEh",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "OHn4Dct23jEm",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# 1. Clean the sentences\n",
    "# 2. Return word pairs in the format: [ENGLISH, FRENCH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code",
    "id": "cTbSbBz55QtF",
    "outputId": "c14045f1-7089-452d-a0ac-625986dc93b8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Je conviens que nous avons besoin d' un agenda social ambitieux qui englobera la lutte contre la pauvreté et l' exclusion sociale . <end>\n",
      "<start> i agree that we need an ambitious social agenda which will include combating poverty and social exclusion <end>\n"
     ]
    }
   ],
   "source": [
    "fr, en = create_dataset(training_data_path, None)\n",
    "print(fr[-1])\n",
    "print(en[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "OmMZQpdO60dt",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "bIOn8RCNDJXG",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='', oov_token=\"<oov>\", lower=False)\n",
    "    lang_tokenizer.fit_on_texts(lang) #Updates internal vocabulary based on a list of texts \n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "cK_EsNtUvRcG",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def roberta_tokenize(text, roberta_path):\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_path)\n",
    "    input_ids = roberta_tokenizer.batch_encode_plus(text,pad_to_max_length=True )[\"input_ids\"]  \n",
    "    return input_ids, roberta_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "arjzw178auwV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, roberta_eng, roberta_fr, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    # targ -> french, inp -> emglish\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_train, input_val, target_train, target_val = train_test_split(inp_lang, targ_lang, test_size=0.2, random_state=1234)\n",
    "\n",
    "    input_tensor_train, inp_lang_tokenizer_train = roberta_tokenize(input_train, roberta_eng)\n",
    "    input_tensor_val, inp_lang_tokenizer_val = roberta_tokenize(input_val, roberta_eng)\n",
    "    \n",
    "    target_tensor_train, targ_lang_tokenizer_train = tokenize(target_train)\n",
    "    target_tensor_val, targ_lang_tokenizer_val = tokenize(target_val)\n",
    "    \n",
    "    return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val, inp_lang_tokenizer_train, inp_lang_tokenizer_val, targ_lang_tokenizer_train, targ_lang_tokenizer_val, input_val, target_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "k-SflKThGSGq",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        print(t)\n",
    "        if t!=0: #if not a padding token\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab_type": "code",
    "id": "nr1Y8ZkFz0Wt",
    "outputId": "6b340b9f-8ff3-4f5e-e6d8-398193c513b2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8800 8800 2200 2200\n"
     ]
    }
   ],
   "source": [
    "#inp_lang and targ_lang are tokenizer objects\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val, inp_lang, inp_lang_val,targ_lang, targ_lang_val, input_gold_val, targ_gold_val = load_dataset(training_data_path, roBERTa_eng_model_path, roberta_fr_model, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_inp_train, max_length_inp_val = max_length(input_tensor_train), max_length(input_tensor_val)\n",
    "max_length_targ_train, max_length_targ_val = max_length(target_tensor_train), max_length(target_tensor_val)\n",
    "\n",
    "max_length_inp = max(max_length_inp_train, max_length_inp_val)\n",
    "\n",
    "max_length_targ = max(max_length_targ_train, max_length_targ_val)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "TqHsArVZ3jFS",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "vocab_inp_size = inp_lang.vocab_size\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab_type": "code",
    "id": "qc6-NK1GtWQt",
    "outputId": "1b128749-a36e-49eb-f2a0-b2643beb783b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  20000\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape\n",
    "mask = example_input_batch != 0\n",
    "mask.shape\n",
    "print(\"Vocab size: \", vocab_inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "_-mupDk0GSHR",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class EncoderWithRoberta(tf.keras.Model):\n",
    "    def __init__(self, roberta_path, embedding_dim, enc_units, batch_sz):\n",
    "        super(EncoderWithRoberta, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        \n",
    "        self.roberta = TFRobertaModel.from_pretrained(roberta_path)\n",
    "        \n",
    "         #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        mask = x != 0\n",
    "        #roBERTa fine-tuned\n",
    "        roberta_hiddens = self.roberta(x, attention_mask=mask)[0] # The last hidden-state is the first element of the output tuple\n",
    "        #roBERTa for feature extraction\n",
    "        #roberta_hiddens = tf.stop_gradient(self.roberta(x)[0], name=\"roberta_hiddens\") # The last hidden-state is the first element of the output tuple\n",
    "        output, state = self.gru(roberta_hiddens, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab_type": "code",
    "id": "60gSVh05Jl6l",
    "outputId": "f32cafba-363c-4633-c56d-3504f685fbf4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made model\n",
      "Encoder output shape: (batch size, sequence length, units) (16, 110, 252)\n",
      "Encoder Hidden state shape: (batch size, units) (16, 252)\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderWithRoberta(roBERTa_eng_model_path, embedding_dim, units, BATCH_SIZE)\n",
    "print(\"made model\")\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "umohpBN2OM94",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab_type": "code",
    "id": "k534zTHiDjQU",
    "outputId": "b60ebc4c-c1d8-4f47-d4ff-6e23cc74ee42",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (16, 252)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (16, 110, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab_type": "code",
    "id": "P5UY8wko3jFp",
    "outputId": "7c58af63-e4ea-4eda-b24e-ec15a5c01f3d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 16297)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "WmTHr5iV3jFr",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab_type": "code",
    "id": "sC9ArXSsVfqn",
    "outputId": "d80b1587-f260-46e8-d8bc-e986039a6a37",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-8f63d5798cc2>:33: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "code",
    "id": "ddefjBMa3jF0",
    "outputId": "98a17f05-e634-4d7b-9511-9f4b5f24e11e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n",
      "True\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "Epoch 1 Batch 0 Loss 1.6368\n",
      "Epoch 1 Batch 100 Loss 1.3989\n",
      "Epoch 1 Batch 200 Loss 1.4259\n",
      "Epoch 1 Batch 300 Loss 1.2599\n",
      "Epoch 1 Batch 400 Loss 1.3759\n",
      "Epoch 1 Batch 500 Loss 1.2188\n",
      "Epoch 1 Loss 1.3455\n",
      "Time taken for 1 epoch 635.9484951496124 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.1031\n",
      "Epoch 2 Batch 100 Loss 1.1485\n",
      "Epoch 2 Batch 200 Loss 1.3118\n",
      "Epoch 2 Batch 300 Loss 1.1515\n",
      "Epoch 2 Batch 400 Loss 1.1731\n",
      "Epoch 2 Batch 500 Loss 1.3768\n",
      "Epoch 2 Loss 1.1599\n",
      "Time taken for 1 epoch 465.74996757507324 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt-1'"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "4WKoXv1dVjv8",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def evaluate(inputs):\n",
    "    sentence = inputs\n",
    "\n",
    "    result = ''\n",
    "    inputs = inp_lang.encode(inputs, return_tensors=\"tf\",pad_to_max_length=True )\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        \n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "5C5oCDyw__2r",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    #print(\"FRENCH TRANSLATION \" + result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab_type": "code",
    "id": "UJpT9D5_OgP6",
    "outputId": "38c81211-d8d9-4969-9326-f79947c795b8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd1b64867b8>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab_type": "code",
    "id": "w_rHzscAGSIn",
    "outputId": "6247a7a6-ec2c-458e-a17b-1cb7c872090c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open(target_gold_path, 'w', encoding='utf-8') as target_file_gold, \\\n",
    "open(target_predicted_path, 'w', encoding='utf-8') as target_file_predicted, \\\n",
    "open(input_gold_path, 'w', encoding='utf-8') as input_file_gold:\n",
    "\n",
    "    for sent in input_gold_val:\n",
    "        count+=1\n",
    "        if count%50==0:\n",
    "            print(count)  \n",
    "        sent = sent.replace('<start> ', \"\")\n",
    "        sent = sent.replace(' <end>', \"\")\n",
    "        sent = sent.replace('<oov>', \"\")\n",
    "        input_file_gold.write(sent.strip() + '\\n') #writing the gold english sentences to file\n",
    "        res = translate(sent)\n",
    "        target_file_predicted.write(res.strip() + '\\n') #writing the predicted french sentences to file \n",
    "    \n",
    "    for line in targ_gold_val:\n",
    "        line = line.replace('<start> ', \"\")\n",
    "        line = line.replace(' <end>', \"\")\n",
    "        line = line.replace('<oov>', \"\")\n",
    "        target_file_gold.write(line.strip() + '\\n') #writing the gold french sentences to file\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "GRUs_Google_roBERTa.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
